import os
import json
import logging
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

PROMPT_PATH = os.getenv('PROMPT_PATH')


def load_prompt(user_question: str) -> str:
    """
    Load prompt template from a text file and fill in the QUESTION placeholder with the user's input.

    :param user_question: The natural language question from the user.
    :return: The filled-in prompt template as a string.
    """
    try:
        with open(PROMPT_PATH, 'r') as file:
            prompt_template = file.read()
        return prompt_template.format(QUESTION=user_question)
    except FileNotFoundError:
        logging.error("Prompt file not found. Ensure the PROMPT_PATH is correct.")
        raise
    except Exception as e:
        logging.error(f"Unexpected error while loading prompt: {e}")
        raise


class LLMHandler:
    def __init__(self, model_name: str, temperature: float, **model_kwargs):
        """
        Initialize the OpenAI model using LangChain.

        :param model_name: The name of the OpenAI model to use (e.g., gpt-3.5-turbo, gpt-4).
        :param temperature: Sampling temperature for the model.
        :param model_kwargs: Additional parameters for the model.
        """
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            logging.error("OpenAI API key is missing. Please set it in the environment or pass it directly.")
            raise ValueError("OpenAI API key is required.")

        self.conversation_history = []  # To maintain chat context
        try:
            # Initialize the model
            self.llm = ChatOpenAI(
                model_name=model_name,
                temperature=temperature,
                openai_api_key=self.api_key,
                **model_kwargs
            )
            logging.info("OpenAI model initialized successfully.")
        except Exception as e:
            logging.error(f"Failed to initialize OpenAI model: {e}")
            raise

    def generate_response(self, prompt: str) -> str:
        """
        Generate a response from the OpenAI model for a given prompt.

        :param prompt: The input text prompt.
        :return: The model's response as a string.
        """
        if not prompt:
            logging.error("Prompt cannot be empty.")
            raise ValueError("Prompt cannot be empty.")

        try:
            response = self.llm.invoke(prompt)
            logging.info("Response successfully generated by the model.")
            return response.content
        except Exception as e:
            logging.error(f"Error generating response: {e}")
            raise

    def chat_with_llm(self, user_input: str) -> dict:
        """
        Engages in a chat with the LLM until a valid MongoDB query is generated.

        :param user_input: The user's natural language input.
        :return: A JSON object with query, sort, and explanation.
        """
        while True:
            try:
                # Load the prompt
                current_prompt = load_prompt(user_input)
                logging.info(f"Prompt loaded for input: {user_input}")

                # Send the prompt to the LLM and get the response
                response = self.generate_response(current_prompt)
                logging.info(f"LLM Response: {response}")

                # Parse the response
                parsed_response = json.loads(response)  # Convert the response to a Python dictionary

                # Validate the response
                if "query" not in parsed_response or "explanation" not in parsed_response:
                    logging.warning("Invalid response format. Expected keys: 'query' and 'explanation'.")
                    raise ValueError("Invalid response format.")

                if parsed_response["query"] == {}:
                    # Log ambiguous query
                    logging.info("Ambiguous query detected. Asking user for clarification.")

                    # Add the explanation to the conversation history
                    self.conversation_history.append({
                        "role": "assistant",
                        "content": parsed_response["explanation"]
                    })

                    # Return the ambiguous response for Streamlit to handle
                    return {"type": "explanation", "message": parsed_response["explanation"]}

                # Valid query generated, return it
                logging.info("Valid query generated successfully.")
                return parsed_response

            except json.JSONDecodeError:
                logging.error("Failed to parse LLM response. Ensure the response is valid JSON.")
                raise
            except Exception as e:
                logging.error(f"Unexpected error during chat: {e}")
                raise e

